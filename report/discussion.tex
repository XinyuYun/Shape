\section{Discussion}

This section discusses the limitations (mainly about the expandability of the system) of our project and how it can be improved.


\subsection{About Image Processing}

\textbf{Currently we do not support images which contains overlapping shapes} due to the fact that dealing with overlapping shapes are very difficult.

Another thing is that for the feature extraction to work smoothly, we have several assumptions. Basically the contour lines in the image cannot be too thin, otherwise OpenCV would have problems finding the desired contour. While it is possible to apply some image transformations before feature extraction, it still remains a quite complicated (and out of the scope of this project) task to handle all kinds of visually valid images. Therefore we will \textbf{only use thick-lined images} in our project.

\subsection{Limited Set of Shape Categories}

Currently we can only recognize between 5 figures. To incorporate more figures into the system, we would have to go through the same workflow all over again. It would be tedious and new questions can arise. For instance we may find our current features are not sufficient to achieve a satisfactory performance. We may have to add new rules, change current rules, or reconstruct some (or even all) fuzzy sets. Although the separation of rule base and inference engine does help with system update, updating rule base alone would be quite time-consuming. Therefore it would be desirable to have \textbf{autonomous rule derivation} and \textbf{autonomous feature discovery}.

\subsubsection{Deriving New Rules}

Since rules and inference are separated in our system, there's no doubt that if we obtain some new rules we could just add them to the system. Therefore the main challenge here is how to derive these rules. Currently, we don't have a fully automatic method to do this. Rules must be derived with human intervention. However, we could \textbf{make this process a lot easier by using a decision tree model to help identify features that have discriminative power}, as used for the fitness function of the genetic algorithm described in section 3.4.

\subsubsection{Discovering New Features}

Another problem is how to generate new features. Currently we use a subset of commonly used features. Of course we could use the whole set and select the useful ones, but that leaves us with two problems: 1. How to identify the most useful features? 2. How to integrate newly found features into our system automaticlly?

With the the first problem, we experiment with genetic algorithms as a way to generate features outside the original feature set. Another (better?) approach is to use the whole feature along with all the derivatives and perform \textbf{feature selection} techniques (e.g. LASSO).

The second problem is actually the rule derivation problem paraphrased. It's very hard to entirely eliminate human intervention from this process. Either we \textbf{incorporate the feature into some existing rule} after validating its discriminative power, or we just \textbf{add a new rule}, then the inferencer would deal with multiple rules itself. Yet \textbf{how to combine outcomes of differnet rules} would become a new ``parameter'' that may need training and tuning.
