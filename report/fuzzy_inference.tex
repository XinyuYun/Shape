\section{Fuzzy Inference}

A simplified Sugeno-style fuzzy inference system is developed with functionalities for fuzzy rule / sets parsing, input fuzzification, autonomous inferencing and output defuzzification.

\subsection{Determining Fuzzy sets}

As mentioned in the previous section, the noises in the image inevitably introduces uncertainties to our system. Therefore the feature values referenced in our rules have to be represented in fuzzy sets.

To determine the \textbf{best fuzzy sets capturing the appropriate degree of uncertainties}, we use the statistics from the training set to find the optimal threshold.

For $Thinness$, we use 3 fuzzy sets for ellipse-like, rectangle-like and triangle-like $Extent$ ratio. For $Extent$, we use 2 fuzzy sets for circle-like and square-like $Thinness$ ratio. The shapes and boundries of these fuzzy sets are determined based on the \textbf{distribution and percentiles of feature values} in the training set. We consider the area between 25th percentile and 75th percentile as the \textbf{high confidence range}, while the remaining area plus a small portion of area adjacent to the extreme values (to tolerate some outliners) are considered as the \textbf{low confidence range}. The corresponding polygons are then constructed based on these two ranges to represent the fuzzy sets. The final fuzzy sets are illustrated in Figure 3 (Figures not drawn to scale).

\begin{figure}

\begin{subfigure}[b]{\columnwidth}
\includegraphics[width=\columnwidth]{Figure_3_Fuzzy_Sets_1.png}
\caption{Fuzzy Set of Extent}\end{subfigure}

\begin{subfigure}[b]{\columnwidth}
\includegraphics[width=\columnwidth]{Figure_4_Fuzzy_Sets_2.png}
\caption{Fuzzy Set of Thinness}
\end{subfigure}

\caption{Fuzzy Sets}

\end{figure}

\subsection{Implementing Inference Engine}

Now that the fuzzy sets are determined, we have to implement the inference engine. But the specific nature of our task requires us to make some adaptations to the the usual fuzzy inference engine.

Firstly, unlike the example from the textbook, the set of possible outputs in our task is \textbf{NOT} a set of linguistic values which can be represented by a series of contiguous intervals. We can't compute a COG-like output and decide which category it falls in, because we can't assign a reasonable order to the shapes.

Secondly, rules derived in the previous section are all of the format:

\textit{IF X IS LIKE A AND Y IS NOT LIKE B THEN Shape IS C}

Note that the consequent of each rule is an assertion about the shape of the input figure and that \textbf{each rule corresponds to the recognition process for one specific shape}. In fact a fuzzy inference system of this kind is essentially \textbf{a flattened decision tree}, with \textbf{each rule acting as a filter to calculate the possibility of one specific shape}. The only difference is that in our expert system rules are separated from inference, making it easily maintainable.

Thus, in order to gain the advantages brought by fuzzy inference without over-complicating our task, we choose to implement \textbf{a simplified Sugeno-style inference engine}. In the final defuzzification stage, we don't compute a weighted average of the rule outputs. Instead we examine all the possible values for the target variable $Shape$ and \textbf{choose the one with the highest possibility as the output}.
